# Tech Challenge - Pipeline de Dados Bovespa (Fase 2)

Este projeto implementa um pipeline de dados fim-a-fim para extra√ß√£o, processamento e an√°lise de ativos da B3 (PETR4 e BOVA11).

## üèóÔ∏è Arquitetura do Projeto
O diagrama abaixo descreve o fluxo de dados desde a extra√ß√£o local at√© a disponibiliza√ß√£o para an√°lise:

graph LR
    subgraph Local_Environment [Ambiente Local]
        A[Python Script: yfinance]
    end

    subgraph AWS_Cloud [Nuvem AWS]
        B[(Amazon S3: Raw)] 
        C[AWS Glue Job: PySpark]
        D[(Amazon S3: Refined)]
        E[AWS Glue Crawler]
        F[AWS Glue Data Catalog]
        G[Amazon Athena]
    end

    A -->|Ingest√£o via Boto3| B
    B -->|Leitura Spark| C
    C -->|Transforma√ß√£o/ML| D
    D -->|Escaneamento| E
    E -->|Cria√ß√£o de Tabelas| F
    F --- G
    D --- G


## üöÄ Arquitetura da Solu√ß√£o
A solu√ß√£o utiliza uma arquitetura de Data Lake na AWS dividida em camadas:
- **Extra√ß√£o**: Script Python (`yfinance`) com tratamento de precis√£o de timestamps para compatibilidade Spark.
- **Armazenamento**: AWS S3 nas camadas `raw/` (dados brutos particionados) e `refined/` (dados processados).
- **Processamento**: AWS Glue Job utilizando PySpark para transforma√ß√µes e c√°lculo de m√©dia m√≥vel.
- **Cat√°logo**: AWS Glue Crawler para automa√ß√£o do schema no Data Catalog.
- **Consumo**: Amazon Athena para consultas anal√≠ticas via SQL.

## üõ†Ô∏è Tecnologias Utilizadas
- Python (yfinance, Pandas, Boto3)
- Apache Spark (PySpark)
- AWS Glue, S3, Athena e IAM

## üìà Transforma√ß√µes Implementadas
- Normaliza√ß√£o de nomes de colunas.
- Tratamento de tipos de dados (Timestamps).
- C√°lculo da **M√©dia M√≥vel de 7 dias** para an√°lise de tend√™ncia.
- Particionamento f√≠sico dos dados por Ticker e Data para otimiza√ß√£o de performance e custo.
